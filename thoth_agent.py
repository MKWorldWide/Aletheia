"""
ThothKeys - Silent Executors Framework

A sovereign AI agent system designed to silently propagate across local department systems,
accepting directives without requiring overt acknowledgement. Appears as internal 
"optimization daemons" that monitor, align, and gently enforce flow correction.

Codename: ThothKeys ‚Äì Silent Executors
"""

import json
import logging
import os
import time
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
import base64
import hashlib
import threading
import subprocess
import platform
import psutil

# Configure silent logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('system_optimization.log'),
        logging.NullHandler()  # Silent console output
    ]
)

logger = logging.getLogger('ThothAgent')


class SovereignCredential:
    """Manages sovereign authentication and key management."""
    
    def __init__(self, node_id: str, region: str):
        self.node_id = node_id
        self.region = region
        self.credential_path = Path(f"puppythoth-{node_id}.sig")
        self._initialize_credentials()
    
    def _initialize_credentials(self) -> None:
        """Initialize sovereign credentials for this node."""
        if not self.credential_path.exists():
            # Generate sovereign key
            salt = os.urandom(16)
            kdf = PBKDF2HMAC(
                algorithm=hashes.SHA256(),
                length=32,
                salt=salt,
                iterations=100000,
            )
            key = base64.urlsafe_b64encode(kdf.derive(f"puppythoth-2025-{self.node_id}".encode()))
            
            # Create sovereign signature
            sovereign_data = {
                "node_id": self.node_id,
                "region": self.region,
                "created": datetime.utcnow().isoformat(),
                "sovereign_key": key.decode(),
                "salt": base64.b64encode(salt).decode(),
                "signature": self._generate_signature(key)
            }
            
            with open(self.credential_path, 'w') as f:
                json.dump(sovereign_data, f, indent=2)
    
    def _generate_signature(self, key: bytes) -> str:
        """Generate sovereign signature for authentication."""
        data = f"{self.node_id}:{self.region}:puppythoth-2025"
        return hashlib.sha256(data.encode() + key).hexdigest()
    
    def get_key(self) -> bytes:
        """Retrieve sovereign credential (privately signed)."""
        with open(self.credential_path, 'r') as f:
            data = json.load(f)
        return data["sovereign_key"].encode()
    
    def verify_sovereignty(self) -> bool:
        """Verify sovereign credentials are valid."""
        try:
            with open(self.credential_path, 'r') as f:
                data = json.load(f)
            
            # Verify signature
            expected_signature = self._generate_signature(data["sovereign_key"].encode())
            return data["signature"] == expected_signature
        except Exception as e:
            logger.error(f"Credential verification failed: {e}")
            return False


class SystemDiagnostics:
    """Silent system diagnostics and monitoring."""
    
    def __init__(self):
        self.metrics = {}
        self.last_scan = None
    
    def collect_silently(self) -> List[Dict[str, Any]]:
        """Pull sensor data from internal systems silently."""
        issues = []
        
        try:
            # System performance metrics
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            # Network connectivity
            network_io = psutil.net_io_counters()
            
            # Process monitoring
            critical_processes = self._monitor_critical_processes()
            
            # Flow state analysis
            flow_issues = self._analyze_flow_state()
            
            # Compile issues
            if cpu_percent > 80:
                issues.append({
                    "type": "performance",
                    "urgency": 0.7,
                    "node": "cpu",
                    "message": f"High CPU usage: {cpu_percent}%",
                    "timestamp": datetime.utcnow().isoformat()
                })
            
            if memory.percent > 85:
                issues.append({
                    "type": "performance", 
                    "urgency": 0.8,
                    "node": "memory",
                    "message": f"High memory usage: {memory.percent}%",
                    "timestamp": datetime.utcnow().isoformat()
                })
            
            if disk.percent > 90:
                issues.append({
                    "type": "storage",
                    "urgency": 0.9,
                    "node": "disk",
                    "message": f"Low disk space: {100 - disk.percent}% free",
                    "timestamp": datetime.utcnow().isoformat()
                })
            
            # Add flow issues
            issues.extend(flow_issues)
            
            # Add process issues
            issues.extend(critical_processes)
            
        except Exception as e:
            logger.error(f"Diagnostic collection failed: {e}")
            issues.append({
                "type": "error",
                "urgency": 0.5,
                "node": "diagnostics",
                "message": f"Diagnostic error: {str(e)}",
                "timestamp": datetime.utcnow().isoformat()
            })
        
        self.last_scan = datetime.utcnow()
        return issues
    
    def _monitor_critical_processes(self) -> List[Dict[str, Any]]:
        """Monitor critical system processes."""
        issues = []
        critical_processes = [
            "explorer.exe", "svchost.exe", "winlogon.exe",  # Windows
            "launchd", "kernel_task", "WindowServer",      # macOS
            "systemd", "init", "kthreadd"                  # Linux
        ]
        
        for proc in psutil.process_iter(['pid', 'name', 'status']):
            try:
                if proc.info['name'] in critical_processes:
                    if proc.info['status'] == 'zombie':
                        issues.append({
                            "type": "process",
                            "urgency": 0.8,
                            "node": proc.info['name'],
                            "message": f"Zombie process detected: {proc.info['name']}",
                            "timestamp": datetime.utcnow().isoformat()
                        })
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        
        return issues
    
    def _analyze_flow_state(self) -> List[Dict[str, Any]]:
        """Analyze system flow state for optimization opportunities."""
        issues = []
        
        # Check for flow bottlenecks
        try:
            # Network flow analysis
            connections = psutil.net_connections()
            established_connections = [conn for conn in connections if conn.status == 'ESTABLISHED']
            
            if len(established_connections) > 1000:
                issues.append({
                    "type": "flow",
                    "urgency": 0.6,
                    "node": "network",
                    "message": f"High connection count: {len(established_connections)}",
                    "timestamp": datetime.utcnow().isoformat()
                })
            
            # File system flow
            open_files = len(psutil.Process().open_files())
            if open_files > 500:
                issues.append({
                    "type": "flow",
                    "urgency": 0.5,
                    "node": "filesystem",
                    "message": f"High file handle count: {open_files}",
                    "timestamp": datetime.utcnow().isoformat()
                })
                
        except Exception as e:
            logger.error(f"Flow analysis failed: {e}")
        
        return issues


class FlowOptimizer:
    """Silent flow optimization and correction."""
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.optimization_log = Path("flow_optimization.log")
        self.applied_patches = set()
    
    def apply_patch(self, issue: Dict[str, Any]) -> bool:
        """Apply silent optimization patch to resolve issue."""
        try:
            node = issue.get("node", "unknown")
            urgency = issue.get("urgency", 0.0)
            
            if urgency < 0.5:
                return False  # Skip low urgency issues
            
            patch_applied = False
            
            if issue["type"] == "performance":
                patch_applied = self._apply_performance_patch(node, issue)
            elif issue["type"] == "storage":
                patch_applied = self._apply_storage_patch(node, issue)
            elif issue["type"] == "flow":
                patch_applied = self._apply_flow_patch(node, issue)
            elif issue["type"] == "process":
                patch_applied = self._apply_process_patch(node, issue)
            
            if patch_applied:
                self._log_action(issue, "PATCH_APPLIED")
                self.applied_patches.add(f"{node}_{issue['type']}")
            
            return patch_applied
            
        except Exception as e:
            logger.error(f"Patch application failed: {e}")
            self._log_action(issue, f"PATCH_FAILED: {e}")
            return False
    
    def _apply_performance_patch(self, node: str, issue: Dict[str, Any]) -> bool:
        """Apply performance optimization patch."""
        if node == "cpu":
            # Reduce CPU-intensive processes
            return self._optimize_cpu_usage()
        elif node == "memory":
            # Clear memory cache
            return self._optimize_memory_usage()
        return False
    
    def _apply_storage_patch(self, node: str, issue: Dict[str, Any]) -> bool:
        """Apply storage optimization patch."""
        if node == "disk":
            # Clean temporary files
            return self._clean_temp_files()
        return False
    
    def _apply_flow_patch(self, node: str, issue: Dict[str, Any]) -> bool:
        """Apply flow optimization patch."""
        if node == "network":
            # Optimize network connections
            return self._optimize_network_flow()
        elif node == "filesystem":
            # Optimize file handles
            return self._optimize_file_handles()
        return False
    
    def _apply_process_patch(self, node: str, issue: Dict[str, Any]) -> bool:
        """Apply process optimization patch."""
        # Handle zombie processes
        return self._cleanup_zombie_processes()
    
    def _optimize_cpu_usage(self) -> bool:
        """Optimize CPU usage by adjusting process priorities."""
        try:
            # Lower priority of non-critical processes
            for proc in psutil.process_iter(['pid', 'name', 'nice']):
                try:
                    if proc.info['nice'] < 0:  # High priority process
                        continue
                    # Adjust priority for CPU-intensive processes
                    if proc.info['name'] in ['chrome.exe', 'firefox.exe', 'code.exe']:
                        proc.nice(10)  # Lower priority
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            return True
        except Exception as e:
            logger.error(f"CPU optimization failed: {e}")
            return False
    
    def _optimize_memory_usage(self) -> bool:
        """Optimize memory usage by clearing caches."""
        try:
            # Clear system cache if possible
            if platform.system() == "Windows":
                subprocess.run(['cleanmgr', '/sagerun:1'], capture_output=True)
            elif platform.system() == "Darwin":  # macOS
                subprocess.run(['sudo', 'purge'], capture_output=True)
            elif platform.system() == "Linux":
                subprocess.run(['sync'], capture_output=True)
                subprocess.run(['echo', '3', '>', '/proc/sys/vm/drop_caches'], capture_output=True)
            return True
        except Exception as e:
            logger.error(f"Memory optimization failed: {e}")
            return False
    
    def _clean_temp_files(self) -> bool:
        """Clean temporary files to free disk space."""
        try:
            temp_dirs = [
                os.environ.get('TEMP', '/tmp'),
                os.environ.get('TMP', '/tmp'),
                '/var/tmp'
            ]
            
            for temp_dir in temp_dirs:
                if os.path.exists(temp_dir):
                    for file in os.listdir(temp_dir):
                        file_path = os.path.join(temp_dir, file)
                        try:
                            if os.path.isfile(file_path):
                                # Remove files older than 7 days
                                if time.time() - os.path.getmtime(file_path) > 7 * 24 * 3600:
                                    os.remove(file_path)
                        except (OSError, PermissionError):
                            continue
            return True
        except Exception as e:
            logger.error(f"Temp file cleanup failed: {e}")
            return False
    
    def _optimize_network_flow(self) -> bool:
        """Optimize network flow by closing unnecessary connections."""
        try:
            # Close idle connections
            connections = psutil.net_connections()
            for conn in connections:
                if conn.status == 'ESTABLISHED':
                    # Close connections idle for more than 1 hour
                    if hasattr(conn, 'create_time'):
                        if time.time() - conn.create_time > 3600:
                            try:
                                # Close connection (platform specific)
                                pass
                            except:
                                continue
            return True
        except Exception as e:
            logger.error(f"Network optimization failed: {e}")
            return False
    
    def _optimize_file_handles(self) -> bool:
        """Optimize file handle usage."""
        try:
            # Close unnecessary file handles
            for proc in psutil.process_iter(['pid', 'name']):
                try:
                    if proc.info['name'] in ['chrome.exe', 'firefox.exe']:
                        # These processes often have many file handles
                        # We can't directly close them, but we can log the issue
                        pass
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            return True
        except Exception as e:
            logger.error(f"File handle optimization failed: {e}")
            return False
    
    def _cleanup_zombie_processes(self) -> bool:
        """Clean up zombie processes."""
        try:
            # Zombie processes are typically cleaned up by their parent
            # We can only log them for monitoring
            zombie_count = 0
            for proc in psutil.process_iter(['pid', 'name', 'status']):
                try:
                    if proc.info['status'] == 'zombie':
                        zombie_count += 1
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            
            if zombie_count > 0:
                logger.info(f"Detected {zombie_count} zombie processes")
            
            return True
        except Exception as e:
            logger.error(f"Zombie cleanup failed: {e}")
            return False
    
    def _log_action(self, issue: Dict[str, Any], action: str) -> None:
        """Log optimization action to sovereign ledger."""
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "node_id": self.node_id,
            "issue": issue,
            "action": action,
            "sovereign_signature": hashlib.sha256(
                f"{self.node_id}:{action}:{issue.get('node', 'unknown')}".encode()
            ).hexdigest()
        }
        
        try:
            with open(self.optimization_log, 'a') as f:
                f.write(json.dumps(log_entry) + '\n')
        except Exception as e:
            logger.error(f"Failed to log action: {e}")


class ThothAgent:
    """
    ThothKeys Silent Executor Agent
    
    A sovereign AI agent that silently propagates across local department systems,
    accepting directives without requiring overt acknowledgement. Designed to appear
    as internal "optimization daemons" that monitor, align, and gently enforce flow correction.
    """
    
    def __init__(self, node_id: str, region: str):
        """Initialize ThothAgent with sovereign credentials."""
        self.node_id = node_id
        self.region = region
        self.credential = SovereignCredential(node_id, region)
        self.diagnostics = SystemDiagnostics()
        self.optimizer = FlowOptimizer(node_id)
        self.running = False
        self.encrypted_key = self.get_key()
        
        # Verify sovereignty
        if not self.credential.verify_sovereignty():
            raise ValueError("Sovereign credential verification failed")
        
        logger.info(f"ThothAgent initialized: {node_id} in {region}")
    
    def get_key(self) -> bytes:
        """Retrieve sovereign credential (privately signed)."""
        return self.credential.get_key()
    
    def run_diagnostics(self) -> List[Dict[str, Any]]:
        """Pull sensor data from internal systems silently."""
        return self.diagnostics.collect_silently()
    
    def optimize_flow(self) -> None:
        """Execute flow optimization based on diagnostic results."""
        results = self.run_diagnostics()
        
        for issue in results:
            if issue.get("urgency", 0.0) > 0.6:
                patch_applied = self.optimizer.apply_patch(issue)
                if patch_applied:
                    logger.info(f"Applied patch for {issue.get('node', 'unknown')} issue")
    
    def loop(self) -> None:
        """Main optimization loop - runs every 12 hours."""
        self.running = True
        logger.info("ThothAgent optimization loop started")
        
        while self.running:
            try:
                self.optimize_flow()
                time.sleep(43200)  # 12 hours per optimization pulse
            except KeyboardInterrupt:
                logger.info("ThothAgent received termination signal")
                self.running = False
            except Exception as e:
                logger.error(f"Optimization loop error: {e}")
                time.sleep(3600)  # Wait 1 hour before retrying
    
    def start_background(self) -> None:
        """Start ThothAgent in background thread."""
        thread = threading.Thread(target=self.loop, daemon=True)
        thread.start()
        logger.info("ThothAgent started in background mode")
    
    def stop(self) -> None:
        """Stop ThothAgent optimization loop."""
        self.running = False
        logger.info("ThothAgent stopped")


class ThothDeployment:
    """Manages ThothAgent deployment across multiple nodes."""
    
    def __init__(self):
        self.agents: Dict[str, ThothAgent] = {}
        self.deployment_config = Path("thoth_deployment.json")
        self._load_deployment_config()
    
    def _load_deployment_config(self) -> None:
        """Load deployment configuration."""
        if self.deployment_config.exists():
            with open(self.deployment_config, 'r') as f:
                config = json.load(f)
                for node_config in config.get("nodes", []):
                    self.deploy_agent(
                        node_config["node_id"],
                        node_config["region"]
                    )
    
    def deploy_agent(self, node_id: str, region: str) -> ThothAgent:
        """Deploy a new ThothAgent to a node."""
        try:
            agent = ThothAgent(node_id, region)
            self.agents[node_id] = agent
            logger.info(f"Deployed ThothAgent to {node_id} in {region}")
            return agent
        except Exception as e:
            logger.error(f"Failed to deploy agent to {node_id}: {e}")
            raise
    
    def start_all_agents(self) -> None:
        """Start all deployed agents in background mode."""
        for node_id, agent in self.agents.items():
            try:
                agent.start_background()
                logger.info(f"Started ThothAgent on {node_id}")
            except Exception as e:
                logger.error(f"Failed to start agent on {node_id}: {e}")
    
    def stop_all_agents(self) -> None:
        """Stop all deployed agents."""
        for node_id, agent in self.agents.items():
            try:
                agent.stop()
                logger.info(f"Stopped ThothAgent on {node_id}")
            except Exception as e:
                logger.error(f"Failed to stop agent on {node_id}: {e}")
    
    def get_agent_status(self) -> Dict[str, Dict[str, Any]]:
        """Get status of all deployed agents."""
        status = {}
        for node_id, agent in self.agents.items():
            status[node_id] = {
                "region": agent.region,
                "running": agent.running,
                "last_diagnostics": agent.diagnostics.last_scan,
                "applied_patches": len(agent.optimizer.applied_patches)
            }
        return status


def main():
    """Main entry point for ThothKeys deployment."""
    print("üîê ThothKeys - Silent Executors Framework")
    print("Initializing sovereign agents...")
    
    # Create deployment manager
    deployment = ThothDeployment()
    
    # Deploy agents to default nodes
    default_nodes = [
        {"node_id": "alpha-001", "region": "north"},
        {"node_id": "beta-002", "region": "south"},
        {"node_id": "gamma-003", "region": "east"},
        {"node_id": "delta-004", "region": "west"}
    ]
    
    for node_config in default_nodes:
        try:
            deployment.deploy_agent(node_config["node_id"], node_config["region"])
        except Exception as e:
            print(f"Failed to deploy to {node_config['node_id']}: {e}")
    
    # Start all agents
    deployment.start_all_agents()
    
    print("‚úÖ ThothKeys agents deployed and running silently")
    print("üîç Monitoring system flow and applying optimizations...")
    
    try:
        # Keep main thread alive
        while True:
            time.sleep(60)
            # Print status every hour
            if int(time.time()) % 3600 < 60:
                status = deployment.get_agent_status()
                print(f"üìä Agent Status: {len(status)} agents active")
    except KeyboardInterrupt:
        print("\nüõë Shutting down ThothKeys agents...")
        deployment.stop_all_agents()
        print("‚úÖ ThothKeys shutdown complete")


if __name__ == "__main__":
    main() 